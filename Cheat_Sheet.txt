ARIMA
# Visualize the Data
from pandas.plotting import lag_plot

lag_plot(series)
series.plot()
#we can see data is non-stationary

#To check the Stationarity

from statsmodels.tsa.stattools import adfuller

def adfuller_test(series):
  result = adfuller(series)
  print(result[0])
  print(result[1])

  if result[1]<=0.05:
    print("The Data is Stationary")
  else:
    print("The Data is Non-Stationary")
adfuller_test(series)


def adfuller_test(series):
  results = adfuller(series.dropna())
  return results[1]

def adfuller_diff(series):
  p_value = adfuller_test(series)

  diff_count = 0
  while p_value > 0.05:
    diff_count += 1
    series = series.diff()
    p_value = adfuller_test(series)

  return diff_count

adfuller_diff(series)


#To check the Stationarity
series = series.diff().dropna()
from statsmodels.tsa.stattools import adfuller
def adfuller_test(series):
  result = adfuller(series)
  print(result[0])
  print(result[1])

  if result[1]<=0.05:
    print("The Data is Stationary")
  else:
    print("The Data is Non-Stationary")
adfuller_test(series)

series.plot()


## Auto Correlation Plot
from pandas.plotting import autocorrelation_plot

autocorrelation_plot(series)
plt.show()


from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1,2,figsize=(12,8))

plot_acf(series.iloc[13:],lags=10,ax=axes[0])
plot_pacf(series.iloc[13:],lags=10,ax=axes[1])

plt.tight_layout()
plt.show()


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from math import sqrt

train_size = int(len(series) * 0.7)
train, test = series[:train_size], series[train_size:]

# Fit ARIMA model (set d=0 if differencing already done)
model = ARIMA(train, order=(1, 1, 1))
model_fit = model.fit()


# Plot diagnostics
model_fit.plot_diagnostics(figsize=(12, 8))
plt.show()

# Make predictions
predictions = model_fit.predict(start=len(train), end=len(series)-1)


# Calculate RMSE
rmse = sqrt(mean_squared_error(test, predictions))
print(f'RMSE: {rmse}')


# Plot actual vs predicted
plt.figure(figsize=(10, 6))
plt.plot(test.index,test, label='Actual', color='blue')
plt.plot(test.index,predictions, color='red', label='Predicted')
plt.title('Actual vs Predicted Values')
plt.xlabel('Time')
plt.ylabel('Values')
plt.legend()
plt.show()


#3 Methods to check stationarity

# 1. Stationarity Check: Plotting
plt.figure(figsize=(12,8))
series.plot(title="To Check the data is Stationary",xlabel="Date",ylabel="Daily Temp")
plt.axhline(y=series.mean(),color="r",linestyle='--',label="Mean")
plt.axhline(y=series.median(),color="g",linestyle='--',label="Median")
plt.legend()
plt.show()

# 1. Stationarity Check: Statistics

split = int(len(series)/2)
x1 , x2 = series[:split], series[split:]
mean1, mean2 = x1.mean() , x2.mean()
var1, var2 = x1.var(), x2.var()

print(mean1," ",mean2)
print(var1, " ",var2)

print("---"*5)

if(abs(mean1 - mean2)<0.05) and (abs(var1 - var2)<1):
  print("May be Stationary")
else:
  print("Not a Stationary")

# 3. Stationarity Check: ADF Test
res = adfuller(ts.values)
print('ADF Statistic: ', res[0])
print('p-value: ', res[1])
for key, value in res[4].items():
    print(f'Critical Value {key}: {value}')

if res[1] <= 0.05:
    print('Reject H0: Time series is stationary')
else:
    print('Do not reject H0: Time series is non-stationary')

#Load and Basic Feature Engineering

#Exploration
print(ts.head(5))
print("---"*10)
print(ts.tail(5))
print("---"*10)
print(ts.isnull().sum())
print("---"*10)
ts.describe()

#Basic Feature Engineering
df = pd.DataFrame({
    'Year':ts.index.year,
    'Month':ts.index.month,
    'Day':ts.index.day,
    'Roberries':ts.values
})


#Rolling Window
t = pd.DataFrame(ts.values)
rolling_windows = t.rolling(window=3).agg(['min','mean','max'])
t = pd.concat([rolling_windows,t],axis=1)
t



#ReSampling
import matplotlib.pyplot as plt
plt.figure(figsize=(8,4))
plt.plot(ts,label='Original Data')
plt.title('Original Data')


plt.figure(figsize=(8,4))
dn_ts = ts.resample('D').ffill()
plt.plot(dn_ts,label='Upsampling',color='red')
plt.title("Upsampling")


plt.figure(figsize=(8,4))
up_ts = ts.resample('ME').ffill()
plt.plot(up_ts,label='DownSampling',color='green')
plt.title('DownSampling')
plt.show()


# Model Evaluation and BaseLine Prediction

#Model Evaluation
ts_diff=ts.diff().dropna()
train_size = int(len(ts_diff)*0.75)
train , test = ts[train_size:],ts[:train_size]

# Walk Forward Validation
n_test = 10
prediction = []

for i in range(len(ts_diff) - n_test):
  train = ts_diff[:i + len(ts_diff)-n_test]
  test = ts_diff[i + len(ts_diff) - n_test:i+ len(ts_diff)]

  print(f'Iteration {i+1}: Train : {len(train)}: Test : {len(test)}')


#ReSampling
import matplotlib.pyplot as plt
plt.figure(figsize=(8,4))
plt.plot(ts,label='Original Data')
plt.title('Original Data')


plt.figure(figsize=(8,4))
dn_ts = ts.resample('D').ffill()
plt.plot(dn_ts,label='Upsampling',color='red')
plt.title("Upsampling")


plt.figure(figsize=(8,4))
up_ts = ts.resample('ME').ffill()
plt.plot(up_ts,label='DownSampling',color='green')
plt.title('DownSampling')
plt.show()

# Model Evaluation and BseLine

train_split = int(len(series)*0.7)
train , test = series[train_split:], series[:train_split]

n_test = len(test)

for i in range(n_test):

  train_subset = test[:train_split+i]

  validate = test[i]

  metric = train_subset.mean()

  print(f"Iteration: {i+1}, Training Set: {len(train_subset)}, Validating Set: {validate}, Metrics: {metric}")

#Baseline Model
train_split = int(len(series))
baseline_predictions = [train.iloc[-1]]*len(test)
rmse = sqrt(mean_squared_error(test.values,baseline_predictions))
rmse

# Plotting the baseline predictions vs actual values
plt.figure(figsize=(10, 6))

# Plot actual test values
plt.plot(test.index, test.values, label='Actual', color='blue', linestyle='-', marker='o')

# Plot baseline predictions
plt.plot(test.index, baseline_predictions, label='Baseline Prediction', color='red', linestyle='--', marker='x')

# Adding labels and title
plt.title('Baseline Predictions vs Actual Test Values')
plt.xlabel('Time')
plt.ylabel('Values')
plt.legend()

# Show the plot
plt.show()

To perform exploratory data analysis (EDA) and prepare the dataset for time series forecasting, we'll follow these steps:

Steps for Exploratory Data Analysis and Data Preparation:

1. Load and Inspect Data: Load the dataset, inspect the data structure, and check for missing values or unwanted columns.


2. Feature Engineering:

Remove unnecessary columns.

Handle missing values.

Aggregate sales data by date (if the dataset has multiple entries for a single day).



3. Data Preparation:

Index the data with the time series.

Handle any outliers or data anomalies.



4. Visualize: Plot the data to better understand trends, seasonality, and patterns.


5. Final Preparation: Prepare the data for modeling by ensuring itâ€™s indexed as a time series.



Sample Code for EDA and Data Preparation:

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and Inspect Data
# Replace with your actual file path
df = pd.read_csv(r'C:\path\to\your\furniture_sales.csv')

# Check the first few rows of the data
print(df.head())

# Check the data types and missing values
print(df.info())

# Step 2: Remove Unwanted Columns
# Assuming 'Sales' and 'Date' are the relevant columns, drop others if necessary
# Replace 'unnecessary_column' with actual column names to drop if needed
df = df.drop(['unnecessary_column_1', 'unnecessary_column_2'], axis=1, errors='ignore')

# Step 3: Handle Missing Values
# Check for missing values
print(df.isnull().sum())

# Fill or drop missing values depending on the context
df['Sales'].fillna(df['Sales'].mean(), inplace=True)  # Example: filling missing sales with mean

# Step 4: Convert Date Column to Datetime Format and Aggregate by Date
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')  # Ensure correct date format

# If there are multiple sales entries per date, aggregate by sum of sales for each date
df = df.groupby('Date').agg({'Sales': 'sum'}).reset_index()

# Step 5: Set Date as Index and Check Time Series Structure
df.set_index('Date', inplace=True)
print(df.head())  # Check if Date is set as the index correctly

# Step 6: Visualize the Time Series Data
plt.figure(figsize=(12,6))
plt.plot(df['Sales'], label='Sales')
plt.title('Furniture Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

# Step 7: Check for Outliers
plt.figure(figsize=(10, 6))
sns.boxplot(df['Sales'])
plt.title('Sales Boxplot (Outlier Detection)')
plt.show()

# Step 8: Final Preparation for Forecasting
# Resample the data if necessary (e.g., monthly or weekly sales)
# Example: Resample to monthly sales
monthly_sales = df['Sales'].resample('M').sum()

# Visualize the resampled data
plt.figure(figsize=(12,6))
plt.plot(monthly_sales, label='Monthly Sales')
plt.title('Monthly Furniture Sales')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

# Data is now ready for applying statistical techniques of forecasting.

Explanation:

1. Load and Inspect Data:

We start by loading the dataset and inspecting the first few rows to understand its structure, such as column names, data types, and any missing values.



2. Feature Engineering:

We drop unwanted columns (like customer IDs, product details, etc.) that are not relevant for forecasting.

We handle missing values either by filling them (with the mean, median, or a forward fill) or by dropping rows if necessary.

If there are multiple sales entries for the same date, we aggregate the sales by date using .groupby() and agg() functions.



3. Data Preparation:

We convert the Date column to a datetime type and set it as the index, which is crucial for time series forecasting.

We resample the data to a monthly frequency if required and visualize the resampled data.



4. Outlier Detection:

We use a boxplot to detect potential outliers in the sales data, which can be addressed later (e.g., capping outliers or removing them).



5. Final Preparation:

The data is now indexed by time, aggregated, and cleaned, making it ready for forecasting. At this stage, we could apply statistical forecasting techniques like ARIMA or exponential smoothing.




Additional Suggestions:

Handle Seasonality and Trends: Depending on the patterns seen in the time series, you may want to decompose the data to handle seasonality and trends before modeling.

Further Aggregation: You can further aggregate data by weeks or months depending on the forecast horizon and data granularity.

Outliers: If significant outliers exist, they might need to be capped or investigated before applying the model.


If you have specific data points or columns that need handling, feel free to share more details for further refinement.

