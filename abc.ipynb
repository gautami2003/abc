{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b4b50",
   "metadata": {},
   "source": [
    "### 1. Text/ Data Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ca2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"Natural Language Processing with Python can be fun and exciting. Let's explore it!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"original words: {tokens}\")\n",
    "print('-'*125)\n",
    "# Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(f\"Filtered Tokens: {filtered_tokens}\")\n",
    "print('-'*125)\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n",
    "print('-'*125)\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(lemmatized_tokens)\n",
    "print(f\"POS Tags: {pos_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341fbf0",
   "metadata": {},
   "source": [
    "### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing with Python can be fun and exciting. Let's explore it!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Original words: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a069b2",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data (only needed once)\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing with Python can be fun and exciting.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(f\"Original words: {tokens}\")\n",
    "print('-'*125)\n",
    "print(f\"Stemmed Tokens: {stemmed_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f67b5",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data (only needed once)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "text = \"Natural Language Processing with Python can be fun and exciting.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(f\"Original words: {tokens}\")\n",
    "print('-'*125)\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00021c9",
   "metadata": {},
   "source": [
    "### 5. Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99747b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "\n",
    "#load the english language module\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#sample text\n",
    "text= \"Natural Language Processing with Python can be fun and exciting.\"\n",
    "\n",
    "#process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "#Display the PoS tagged result\n",
    "print(\"Original Text: \",text)\n",
    "print('-'*125)\n",
    "print(\"PoS Tagging Result: \")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}:{token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad39fb6",
   "metadata": {},
   "source": [
    "### 6. WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfffb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download necessary NLTK data (only needed once)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Load text from a CSV file\n",
    "#df = pd.read_csv('data.csv')  # Ensure 'data.csv' is in the same directory\n",
    "# Assuming the CSV has a column named 'text'\n",
    "#csv_text = ' '.join(df['text'].astype(str))  # Join all text entries into a single string\n",
    "\n",
    "# Additional text provided in the code\n",
    "user_text = '''Natural Language Processing with Python can be fun and exciting. \n",
    "It enables computers to understand human language and perform various tasks, \n",
    "from translation to sentiment analysis. Exploring NLP opens up many opportunities.'''\n",
    "\n",
    "# Combine both texts\n",
    "#combined_text = csv_text + \" \" + user_text\n",
    "\n",
    "# Tokenization\n",
    "#tokens = word_tokenize(combined_text.lower())\n",
    "tokens = word_tokenize(user_text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Count frequencies of each word\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the Word Cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2d9c4",
   "metadata": {},
   "source": [
    "### 7. Emojification and Demojification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import demoji\n",
    "\n",
    "# Load demoji data for emoji processing (run this only once)\n",
    "#demoji.download_codes()\n",
    "\n",
    "# Define functions for demojification and emoji removal\n",
    "def demojify(text):\n",
    "    \"\"\"Convert emojis in text to their textual representation.\"\"\"\n",
    "    return emoji.demojize(text)\n",
    "def emojify(text):\n",
    "    \"\"\"Convert text with emoji aliases to actual emojis.\"\"\"\n",
    "    return emoji.emojize(text)\n",
    "# Sample text for demonstration\n",
    "sample_texts = [\"Hello :face_with_rolling_eyes: ! I code on my own :beaming_face_with_smiling_eyes:\",\n",
    "               \"I love programming! ðŸš€ Let's build amazing things! ðŸŒŸ\"]\n",
    "# Process sample texts\n",
    "for original_text in sample_texts:\n",
    "    print(f\"Original Text: {original_text}\")\n",
    "    #print('-'*125)\n",
    "    # Emojification\n",
    "    emojified_text = emojify(original_text)\n",
    "    print(f\"Emojified Text: {emojified_text}\")\n",
    "    #print('-'*125)\n",
    "    # Demojification\n",
    "    demojified_text = demojify(original_text)\n",
    "    print(f\"Demojified Text: {demojified_text}\\n\")\n",
    "    print('-'*125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ec106",
   "metadata": {},
   "source": [
    "###  Emojification and Demojification with csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6efdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "# Define functions for demojification and emojification\n",
    "def demojify(text):\n",
    "    \"\"\"Convert emojis in text to their textual representation.\"\"\"\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def emojify(text):\n",
    "    \"\"\"Convert text with emoji aliases to actual emojis.\"\"\"\n",
    "    return emoji.emojize(text)\n",
    "\n",
    "# CSV Processing\n",
    "try:\n",
    "    # Load text from a CSV file\n",
    "    df = pd.read_csv('data.csv')  # Ensure 'data.csv' is in the same directory\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for original_text in df['text']:  # Assumes the CSV has a 'text' column\n",
    "        # Emojification\n",
    "        emojified_text = emojify(original_text)\n",
    "        \n",
    "        # Demojification\n",
    "        demojified_text = demojify(original_text)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Original Text: {original_text}\")\n",
    "        print(f\"Emojified Text: {emojified_text}\")\n",
    "        print(f\"Demojified Text: {demojified_text}\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Please ensure 'data.csv' is in the same directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bf6e8",
   "metadata": {},
   "source": [
    "### 8. Sentiment Analysis - using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"I absolutely loved the new restaurant downtown! The food was amazing, and the service was excellent.\",\n",
    "          \"The movie was fantastic! The acting was top-notch, and the storyline was incredibly engaging.\",\n",
    "          \"I had a wonderful experience with the customer support team. They were prompt and resolved my issue quickly.\",\n",
    "          \"The product I received was defective and didn't work as advertised. I'm very disappointed.\",\n",
    "          \"I had a terrible experience at the hotel. The room was dirty, and the staff was unhelpful.\",\n",
    "          \"The concert was a huge letdown. The sound quality was poor, and the performance was lackluster.\",\n",
    "          \"The book was well-written, but it didn't really capture my interest.\",\n",
    "          \"I received my package on time, but the packaging was a bit damaged.\",\n",
    "          \"The workshop was informative, but it was a bit too long for my liking.\"\n",
    "         ]\n",
    "\n",
    "df = pd.DataFrame(corpus,columns=['text'])\n",
    "\n",
    "def get_polarity_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity,blob.sentiment.subjectivity\n",
    "def get_analysis(score):\n",
    "    if score>0:\n",
    "        return \"Positive\"\n",
    "    elif score==0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "df[['polarity','subjectivity']] = df.text.apply(get_polarity_score).apply(pd.Series)\n",
    "df['analysis'] = df['polarity'].apply(get_analysis)\n",
    "display(df)\n",
    "\n",
    "#plotting\n",
    "# Calculate the percentage of each sentiment\n",
    "sentiment_counts = df['analysis'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = ['gold', 'pink', 'red']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice (Positive)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.title('Sentiment Analysis Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba98fe9",
   "metadata": {},
   "source": [
    "### sentiment analysis using SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "corpus = [\"I absolutely loved the new restaurant downtown! The food was amazing, and the service was excellent.\",\n",
    "          \"The movie was fantastic! The acting was top-notch, and the storyline was incredibly engaging.\",\n",
    "          \"I had a wonderful experience with the customer support team. They were prompt and resolved my issue quickly.\",\n",
    "          \"The product I received was defective and didn't work as advertised. I'm very disappointed.\",\n",
    "          \"I had a terrible experience at the hotel. The room was dirty, and the staff was unhelpful.\",\n",
    "          \"The concert was a huge letdown. The sound quality was poor, and the performance was lackluster.\",\n",
    "          \"The book was well-written, but it didn't really capture my interest.\",\n",
    "          \"I received my package on time, but the packaging was a bit damaged.\",\n",
    "          \"The workshop was informative, but it was a bit too long for my liking.\"\n",
    "         ]\n",
    "\n",
    "df = pd.DataFrame(corpus,columns=['text'])\n",
    "def get_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)\n",
    "df[\"sentiment_score\"] = df.text.apply(get_sentiment_score)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad59f7",
   "metadata": {},
   "source": [
    "### sentiment analysis using SentimentIntensityAnalyzer with plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I absolutely loved the new restaurant downtown! The food was amazing, and the service was excellent.\",\n",
    "    \"The movie was fantastic! The acting was top-notch, and the storyline was incredibly engaging.\",\n",
    "    \"I had a wonderful experience with the customer support team. They were prompt and resolved my issue quickly.\",\n",
    "    \"The product I received was defective and didn't work as advertised. I'm very disappointed.\",\n",
    "    \"I had a terrible experience at the hotel. The room was dirty, and the staff was unhelpful.\",\n",
    "    \"The concert was a huge letdown. The sound quality was poor, and the performance was lackluster.\",\n",
    "    \"The book was well-written, but it didn't really capture my interest.\",\n",
    "    \"I received my package on time, but the packaging was a bit damaged.\",\n",
    "    \"The workshop was informative, but it was a bit too long for my liking.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(corpus, columns=['text'])\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "# Apply the sentiment score function\n",
    "df[\"sentiment_score\"] = df.text.apply(get_sentiment_score)\n",
    "\n",
    "# Extract compound score for simplicity\n",
    "df['compound'] = df['sentiment_score'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "# Define a function to categorize sentiment based on compound score\n",
    "def analyze_sentiment(compound):\n",
    "    if compound >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to categorize sentiments\n",
    "df['analysis'] = df['compound'].apply(analyze_sentiment)\n",
    "\n",
    "# Calculate the percentage of each sentiment\n",
    "sentiment_counts = df['analysis'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plot the sentiment distribution as a pie chart\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = ['gold', 'lightblue', 'salmon']\n",
    "\n",
    "# Explode the Positive slice for emphasis\n",
    "explode = [0.1 if label == 'Positive' else 0 for label in labels]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Sentiment Analysis Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f7d94",
   "metadata": {},
   "source": [
    "### sentiment analysis with csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Lavanyabh\\Desktop\\Sem3\\NLP\\datasets\\Tweets.csv\")\n",
    "\n",
    "df.drop(columns = ['textID','selected_text','sentiment'],inplace = True)\n",
    "\n",
    "#Function to get polarity and sunbjectivity using TextBlob\n",
    "def get_polarity_subjectivity(df):\n",
    "    blob = TextBlob(str(df))\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "#function to get sentiment analysis based on polarity\n",
    "def get_analysis(score):\n",
    "    if score < 0:\n",
    "         return 'Negative'\n",
    "    elif score == 0:\n",
    "         return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "#Apply the function to the DataFrame\n",
    "df[['polarity','subjectivity']] = df.text.apply(get_polarity_subjectivity).apply(pd.Series)\n",
    "\n",
    "#Apply the get_analysis function to get sentiment analysis\n",
    "df[\"analysis\"]=df[\"polarity\"].apply(get_analysis)\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "#plotting\n",
    "# Calculate the percentage of each sentiment\n",
    "sentiment_counts = df['analysis'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = ['gold', 'pink', 'red']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice (Positive)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.title('Sentiment Analysis Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5204a",
   "metadata": {},
   "source": [
    "### 9. AFINN Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c26e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Afinn for sentiment analysis\n",
    "afinn = Afinn()\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I absolutely loved the new restaurant downtown! The food was amazing, and the service was excellent.\",\n",
    "    \"The movie was fantastic! The acting was top-notch, and the storyline was incredibly engaging.\",\n",
    "    \"I had a wonderful experience with the customer support team. They were prompt and resolved my issue quickly.\",\n",
    "    \"The product I received was defective and didn't work as advertised. I'm very disappointed.\",\n",
    "    \"I had a terrible experience at the hotel. The room was dirty, and the staff was unhelpful.\",\n",
    "    \"The concert was a huge letdown. The sound quality was poor, and the performance was lackluster.\",\n",
    "    \"The book was well-written, but it didn't really capture my interest.\",\n",
    "    \"I received my package on time, but the packaging was a bit damaged.\",\n",
    "    \"The workshop was informative, but it was a bit too long for my liking.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(corpus, columns=['text'])\n",
    "\n",
    "# Apply Afinn sentiment analysis\n",
    "df['afinn_score'] = df['text'].apply(afinn.score)\n",
    "\n",
    "# Classify sentiment based on the score\n",
    "def analyze_afinn_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['analysis'] = df['afinn_score'].apply(analyze_afinn_sentiment)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n",
    "# Plotting the sentiment distribution as a pie chart\n",
    "sentiment_counts = df['analysis'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plot the sentiment distribution as a pie chart\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = ['gold', 'lightblue', 'salmon']\n",
    "\n",
    "# Explode the Positive slice for emphasis\n",
    "explode = [0.1 if label == 'Positive' else 0 for label in labels]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('AFINN Sentiment Analysis Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324aa78f",
   "metadata": {},
   "source": [
    "### Afinn Sentiment Analysis with csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Afinn for sentiment analysis\n",
    "afinn = Afinn()\n",
    "# Load CSV file (replace 'your_file.csv' with the path to your file)\n",
    "df = pd.read_csv(r\"C:\\Users\\Lavanyabh\\Desktop\\Sem3\\NLP\\datasets\\Tweets.csv\")\n",
    "# Check for missing values and convert non-strings to empty strings\n",
    "df['text'] = df['text'].fillna('').astype(str)\n",
    "# Apply Afinn sentiment analysis on the 'text' column\n",
    "df['afinn_score'] = df['text'].apply(afinn.score)\n",
    "# Classify sentiment based on the score\n",
    "def analyze_afinn_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "df['analysis'] = df['afinn_score'].apply(analyze_afinn_sentiment)\n",
    "# Display the DataFrame (optional, for verification)\n",
    "display(df[['text', 'afinn_score', 'analysis']])\n",
    "\n",
    "# Plotting the sentiment distribution as a pie chart\n",
    "sentiment_counts = df['analysis'].value_counts(normalize=True) * 100\n",
    "# Plot the sentiment distribution as a pie chart\n",
    "labels = sentiment_counts.index\n",
    "sizes = sentiment_counts.values\n",
    "colors = ['gold', 'lightblue', 'salmon']\n",
    "# Explode the Positive slice for emphasis\n",
    "explode = [0.1 if label == 'Positive' else 0 for label in labels]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('AFINN Sentiment Analysis Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e8921",
   "metadata": {},
   "source": [
    "### 10. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363df7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model for English (small model)\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to extract named entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Sample text for NER\n",
    "sample_text = \"Apple is looking at buying U.K. startup for $1 billion. Elon Musk, the CEO of Tesla, tweeted on Tuesday.\"\n",
    "\n",
    "# Perform NER on the sample text\n",
    "doc = nlp(sample_text)\n",
    "print(\"Named Entities in sample text:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# --- Named Entity Recognition from CSV file ---\n",
    "# Assuming the CSV file has a column 'text' containing sentences or documents\n",
    "#df = pd.read_csv(r\"C:\\Users\\Lavanyabh\\Desktop\\Sem3\\NLP\\datasets\\Tweets.csv\")\n",
    "# Apply NER to each row in the 'text' column\n",
    "#df['entities'] = df['text'].apply(lambda x: extract_entities(str(x)))\n",
    "# Display the results\n",
    "#print(\"\\nNamed Entities in CSV file:\")\n",
    "#print(df[['text', 'entities']])\n",
    "# Save the results to a new CSV file (optional)\n",
    "#df.to_csv(r\"C:\\Users\\Lavanyabh\\Desktop\\Sem3\\NLP\\datasets\\\\save_entities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "061ee94b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "ORG: Organization (e.g., Apple, AI)\n",
    "GPE: Geopolitical Entity (e.g., U.K.)\n",
    "MONEY: Monetary value (e.g., $1 billion)\n",
    "PERSON: Person's name (e.g., John Doe, Jane Smith)\n",
    "DATE: Specific dates or periods (e.g., 2018, the end of the year)\n",
    "WORK_OF_ART: Works of art, which can sometimes capture complex technical terms like \"machine learning algorithms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Hugging Face NER pipeline\n",
    "ner_transformers = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Define a function for spaCy NER\n",
    "def perform_spacy_ner(text):\n",
    "    \"\"\"Apply NER using spaCy and return the entities found in the text.\"\"\"\n",
    "    doc = nlp_spacy(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Define a function for Transformers NER\n",
    "def perform_transformers_ner(text):\n",
    "    \"\"\"Apply NER using Transformers and return the entities found in the text.\"\"\"\n",
    "    entities = ner_transformers(text)\n",
    "    return [(ent['word'], ent['entity_group']) for ent in entities]\n",
    "\n",
    "# Sample text data (can be replaced with a CSV file)\n",
    "texts = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Apple Inc. is based in Cupertino, California.\",\n",
    "    \"Mount Everest is the highest mountain in the world.\",\n",
    "    \"The World War II ended in 1945.\",\n",
    "    \"The iPhone 12 was released by Apple in 2020.\",\n",
    "    \"To Kill a Mockingbird is a famous novel.\",\n",
    "    \"NASA launched the Mars Rover in 2021.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"I paid $500 for a new laptop.\",\n",
    "    \"The Eiffel Tower is 324 meters tall.\",\n",
    "    \"She was born on July 4th, 1990.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame to store the texts and their corresponding entities\n",
    "df = pd.DataFrame(texts, columns=['text'])\n",
    "\n",
    "# Apply the NER functions to the DataFrame\n",
    "df['spacy_entities'] = df['text'].apply(perform_spacy_ner)\n",
    "df['transformers_entities'] = df['text'].apply(perform_transformers_ner)\n",
    "\n",
    "# Display the DataFrame with the recognized entities\n",
    "print(df)\n",
    "\n",
    "# Optional: Visualize one of the NER outputs using spaCy's displacy\n",
    "# You can visualize any example text with NER highlighted\n",
    "doc = nlp_spacy(texts[0])  # Change index to visualize a different text\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dbe80",
   "metadata": {},
   "source": [
    "### 11. Similarity Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983a819",
   "metadata": {},
   "source": [
    "### words similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"!python -m spacy download en_core_web_md \"\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "def word_similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two words using spaCy's pre-trained model.\n",
    "    \"\"\"\n",
    "    token1 = nlp(word1)\n",
    "    token2 = nlp(word2)\n",
    "    return token1.similarity(token2)\n",
    "\n",
    "word1 = \"teacher\"\n",
    "word2 = \"instructor\"\n",
    "word_sim = word_similarity(word1, word2)\n",
    "\n",
    "print(f\"Similarity between words '{word1}' and '{word2}': {word_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a57db",
   "metadata": {},
   "source": [
    "### sentence similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained model\n",
    "nlp = spacy.load('en_core_web_md')  # 'en_core_web_md' is a medium-sized English model with word vectors\n",
    "\n",
    "def sentence_similarity(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two sentences using spaCy's pre-trained model.\n",
    "    \"\"\"\n",
    "    doc1 = nlp(s1)\n",
    "    doc2 = nlp(s2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Example usage\n",
    "s1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "s2 = \"A fast dark fox leaps over a sleepy dog.\"\n",
    "sent_sim = sentence_similarity(s1, s2)\n",
    "\n",
    "print(f\"Similarity between sentences:\\n'{s1}'\\nand\\n'{s2}': {sent_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef434a2",
   "metadata": {},
   "source": [
    "### word and sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure you have the required libraries\n",
    "# You may need to run the following command if you haven't already downloaded the spaCy model:\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def word_similarity_spacy(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two words using spaCy's pre-trained model.\n",
    "    \"\"\"\n",
    "    token1 = nlp(word1)\n",
    "    token2 = nlp(word2)\n",
    "    return token1.similarity(token2)\n",
    "\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two sentences using spaCy's pre-trained model.\n",
    "    \"\"\"\n",
    "    doc1 = nlp(sentence1)\n",
    "    doc2 = nlp(sentence2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Example usage for word similarity\n",
    "word1 = \"teacher\"\n",
    "word2 = \"instructor\"\n",
    "\n",
    "# Similarity using spaCy\n",
    "word_sim_spacy = word_similarity_spacy(word1, word2)\n",
    "print(f\"Similarity between words '{word1}' and '{word2}' (spaCy): {word_sim_spacy:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage for sentence similarity\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"A fast dark fox leaps over a sleepy dog.\"\n",
    "\n",
    "sent_sim = sentence_similarity(sentence1, sentence2)\n",
    "print(f\"Similarity between sentences:\\n'{sentence1}'\\nand\\n'{sentence2}': {sent_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e333e2b",
   "metadata": {},
   "source": [
    "### 12. Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14d5bc",
   "metadata": {},
   "source": [
    "### spam detection with csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ffcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check wherter the incoming sms is spam or not spam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "#1. Data loading\n",
    "data = pd.read_csv(r'C:\\Users\\Lavanyabh\\Desktop\\Sem3\\NLP\\datasets\\spam.csv', encoding='ISO-8859-1')\n",
    "#selecting only relevant columns\n",
    "data = data[['v1','v2']]\n",
    "display(data)\n",
    "#renaming columns for clarity\n",
    "data.columns =  ['label','message']\n",
    "display(data)\n",
    "# 2. pre processing\n",
    "data['label'] = data['label'].map({'ham':0,'spam':1})\n",
    "display(data)\n",
    "#3. feature extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['message'])\n",
    "y = data['label']\n",
    "#4. Train-Test Split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "# 5. Model Training\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "# 6. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "#testing\n",
    "# 7. Function to Predict Spam or Ham\n",
    "def predict_message(text):\n",
    " text_transformed = vectorizer.transform([text])\n",
    " prediction = model.predict(text_transformed)\n",
    " return 'spam' if prediction[0] == 1 else 'ham'\n",
    "\n",
    "# Example Usage\n",
    "sample_text = \"Congratulations! You've won a free ticket to Bahamas. Call now to claim.\"\n",
    "prediction = predict_message(sample_text)\n",
    "print(f\"The message '{sample_text}' is classified as: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7b9f6",
   "metadata": {},
   "source": [
    "### spam detection with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b542537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. Sample Data Creation\n",
    "data = pd.DataFrame({\n",
    "    'label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam'],\n",
    "    'message': [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Congratulations! You've won a free ticket to Bahamas. Call now to claim.\",\n",
    "        \"Are we still meeting tomorrow?\",\n",
    "        \"You have been selected for a chance to get a free iPhone!\",\n",
    "        \"Your appointment is confirmed.\",\n",
    "        \"Free money!!! Click this link to claim your prize.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# 2. Renaming columns for clarity\n",
    "data.columns = ['label', 'message']\n",
    "\n",
    "# 3. Preprocessing\n",
    "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# 4. Feature extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['message'])\n",
    "y = data['label']\n",
    "\n",
    "# 5. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Model Training\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# 8. Function to Predict Spam or Ham\n",
    "def predict_message(text):\n",
    "    text_transformed = vectorizer.transform([text])\n",
    "    prediction = model.predict(text_transformed)\n",
    "    return 'spam' if prediction[0] == 1 else 'ham'\n",
    "\n",
    "# Example Usage\n",
    "sample_text = \"Congratulations! You've won a free ticket to Bahamas. Call now to claim.\"\n",
    "prediction = predict_message(sample_text)\n",
    "print(f\"The message '{sample_text}' is classified as: {prediction}\")\n",
    "\n",
    "# Testing additional messages\n",
    "additional_messages = [\n",
    "    \"Let's schedule a meeting next week.\",\n",
    "    \"You've been selected for a $1000 cash prize!\",\n",
    "    \"Can you send me the report by tomorrow?\"\n",
    "]\n",
    "\n",
    "for msg in additional_messages:\n",
    "    result = predict_message(msg)\n",
    "    print(f\"The message '{msg}' is classified as: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be92356",
   "metadata": {},
   "source": [
    "### 13. Grammar Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d38b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "def grammar_check(text):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    matches = tool.check(text)\n",
    "    #print(matches)\n",
    "    \n",
    "    print(\"Original Text: \")\n",
    "    print(text)\n",
    "    \n",
    "    corrected_text = tool.correct(text)\n",
    "    print(\"Corrected text: \")\n",
    "    print(corrected_text)\n",
    "    \n",
    "text = \"This is a example of a sentence with a error. Their is a problem with grammar.\"\n",
    "grammar_check(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85543c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "# Initialize the LanguageTool object\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def check_grammar(text):\n",
    "    # Check for grammar and spelling errors\n",
    "    matches = tool.check(text)\n",
    "    return matches\n",
    "\n",
    "# Example text to check\n",
    "text = \"This is a example of a sentence with a error. Their is a problem with grammar.\"\n",
    "\n",
    "# Check grammar\n",
    "errors = check_grammar(text)\n",
    "\n",
    "# Display the results\n",
    "if errors:\n",
    "    print(f\"Found {len(errors)} error(s):\")\n",
    "    for error in errors:\n",
    "        print(f\" - Error: {error.context}\")\n",
    "        print(f\"   Suggestion: {error.replacements}\")\n",
    "        print(f\"   Message: {error.message}\")\n",
    "else:\n",
    "    print(\"No grammatical errors found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040e7c0",
   "metadata": {},
   "source": [
    "### 14. N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b52891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK data files (if you haven't done so)\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from the input text.\n",
    "\n",
    "    :param text: The input text (string).\n",
    "    :param n: The number of words in each n-gram.\n",
    "    :return: A list of n-grams.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = ngrams(tokens, n)\n",
    "\n",
    "    return list(n_grams)\n",
    "\n",
    "def count_ngrams(ngrams_list):\n",
    "    \"\"\"\n",
    "    Count occurrences of n-grams.\n",
    "\n",
    "    :param ngrams_list: A list of n-grams.\n",
    "    :return: A Counter object with n-grams as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    return Counter(ngrams_list)\n",
    "\n",
    "# Example usage\n",
    "text = \"I love natural language processing. Natural language processing is fascinating.\"\n",
    "n = 2  # Change this value for different n-grams (e.g., 1 for unigrams, 2 for bigrams, etc.)\n",
    "\n",
    "# Generate n-grams\n",
    "ngrams_list = generate_ngrams(text, n)\n",
    "\n",
    "# Count n-grams\n",
    "ngrams_count = count_ngrams(ngrams_list)\n",
    "\n",
    "# Display the n-grams and their counts\n",
    "print(f\"{n}-grams:\")\n",
    "for gram, count in ngrams_count.items():\n",
    "    print(f\"{gram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6b8c6",
   "metadata": {},
   "source": [
    "### 15. Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa31383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files (if you haven't done so)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Sample data: Replace this with your own dataset or CSV file\n",
    "documents = [\n",
    "    \"I love reading about artificial intelligence and machine learning.\",\n",
    "    \"Natural language processing is a fascinating field of study.\",\n",
    "    \"Deep learning techniques are widely used in computer vision.\",\n",
    "    \"Artificial intelligence can help automate many tasks.\",\n",
    "    \"The future of AI is very promising with new advancements.\"\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Prepare data\n",
    "processed_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Build the LDA model\n",
    "num_topics = 2  # Adjust the number of topics as needed\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Display the topics\n",
    "topics = lda_model.print_topics(num_words=3)  # Change num_words to display more/less words per topic\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Example of assigning topics to documents\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i + 1}: {documents[i]}\")\n",
    "    doc_topics = lda_model.get_document_topics(doc)\n",
    "    print(\"Assigned Topics:\", doc_topics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c7c7917",
   "metadata": {},
   "source": [
    "The topic modeling output using Latent Dirichlet Allocation (LDA) identifies two topics: Topic 0, focusing on \"learning,\" \"artificial,\" and \"intelligence,\" and Topic 1, centered on \"new,\" \"fascinating,\" and \"advancements.\" The document assignments show that Document 1 (about AI and machine learning) has a strong relevance to Topic 1 (89.98%), while Document 2 (on natural language processing) is highly associated with Topic 1 (92.49%). Document 3 (discussing deep learning) is primarily linked to Topic 0 (93.29%), as is Document 4 (about AI in automation, 91.91%). Finally, Document 5 (on AI advancements) is also closely related to Topic 1 (91.24%), indicating a significant emphasis on advancements in AI across the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2907e8",
   "metadata": {},
   "source": [
    "### 16. Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93788496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Sample data\n",
    "choices = [\n",
    "    \"apple pie\",\n",
    "    \"apple tart\",\n",
    "    \"banana split\",\n",
    "    \"cherry pie\",\n",
    "    \"chocolate cake\"\n",
    "]\n",
    "\n",
    "# Single string comparison\n",
    "str1 = \"apple\"\n",
    "str2 = \"apple pie\"\n",
    "similarity_score = fuzz.ratio(str1, str2)\n",
    "print(f\"Similarity between '{str1}' and '{str2}': {similarity_score}%\")\n",
    "\n",
    "# Partial string matching\n",
    "str3 = \"appl\"\n",
    "partial_score = fuzz.partial_ratio(str1, str3)\n",
    "print(f\"Partial similarity between '{str1}' and '{str3}': {partial_score}%\")\n",
    "\n",
    "# Find the best match in a list\n",
    "query = \"apple\"\n",
    "best_match = process.extractOne(query, choices)\n",
    "print(f\"The best match for '{query}' is '{best_match[0]}' with a score of {best_match[1]}%\")\n",
    "\n",
    "# Example for multiple matches\n",
    "results = process.extract(query, choices, limit=3)\n",
    "print(f\"Top 3 matches for '{query}':\")\n",
    "for match in results:\n",
    "    print(f\"{match[0]} - {match[1]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7060fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c897fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import process, fuzz\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file containing company names.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame with company names.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def standardize_company_names(df, threshold=80):\n",
    "    \"\"\"\n",
    "    Standardize company names in a DataFrame using fuzzy matching.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): A DataFrame containing a column of company names.\n",
    "    threshold (int): The minimum score to consider a match (default is 80).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with an additional column for standardized company names.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to map original names to standardized names\n",
    "    standard_name_map = {}\n",
    "\n",
    "    # Iterate over each company name\n",
    "    for name in df['Company Name']:\n",
    "        if name not in standard_name_map:\n",
    "            # Find the best match for the current name within existing names in the map\n",
    "            match_result = process.extractOne(name, standard_name_map.keys(), scorer=fuzz.partial_ratio, score_cutoff=threshold)\n",
    "            \n",
    "            if match_result:\n",
    "                match, _ = match_result\n",
    "                standard_name_map[name] = standard_name_map[match]\n",
    "            else:\n",
    "                standard_name_map[name] = name\n",
    "\n",
    "    # Replace the names in the DataFrame with the standardized names\n",
    "    df['Standardized Company Name'] = df['Company Name'].map(standard_name_map)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example Usage\n",
    "file_path = 'company_names.csv'  # Replace with your CSV file path\n",
    "df = load_data_from_csv(file_path)\n",
    "\n",
    "# Ensure the DataFrame has a column named 'Company Name'\n",
    "if 'Company Name' in df.columns:\n",
    "    df_standardized = standardize_company_names(df)\n",
    "    print(\"\\nStandardized Data:\\n\", df_standardized)\n",
    "else:\n",
    "    print(\"The DataFrame does not contain a column named 'Company Name'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03169e40",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e3dfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between sentence 1 and sentence 2: 0.26055567105626243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sentence_1 = \"I love programming in Python\"\n",
    "sentence_2 = \"Python is a great language for programming\"\n",
    "sentences = [sentence_1, sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "print(f\"Cosine Similarity between sentence 1 and sentence 2: {cosine_sim[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436919e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
