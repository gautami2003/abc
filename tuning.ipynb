{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras, keras-tuner"
      ],
      "metadata": {
        "id": "fWIAQWUNQxX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the IRIS dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Encode labels to one-hot encoding\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Perform Exploratory Data Analysis (EDA)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=X)\n",
        "plt.title(\"Feature Distribution of IRIS Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the model-building function for hyperparameter tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer with number of neurons as a hyperparameter\n",
        "    model.add(Dense(units=hp.Int('units_input', min_value=8, max_value=64, step=8),\n",
        "                    activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    # Hidden layer with tunable neurons\n",
        "    model.add(Dense(units=hp.Int('units_hidden', min_value=8, max_value=64, step=8),\n",
        "                    activation='relu'))\n",
        "\n",
        "    # Dropout layer to reduce overfitting\n",
        "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Output layer (since IRIS dataset has 3 classes, we use softmax activation)\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model with tunable learning rate\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Float('learning_rate',\n",
        "                                                                         min_value=0.001,\n",
        "                                                                         max_value=0.01,\n",
        "                                                                         step=0.001)),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the Keras tuner with Hyperband search strategy\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=20,\n",
        "                     factor=3,\n",
        "                     directory='hyper_tuning',\n",
        "                     project_name='iris_ann')\n",
        "\n",
        "# Define early stopping to prevent overfitting\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "# Get the best model\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Optimal Hyperparameters:\\n\"\n",
        "      f\"Input Layer Units: {best_hps.get('units_input')}\\n\"\n",
        "      f\"Hidden Layer Units: {best_hps.get('units_hidden')}\\n\"\n",
        "      f\"Dropout Rate: {best_hps.get('dropout_rate')}\\n\"\n",
        "      f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Build and train the best model\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the final model\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "# Test the model by making predictions\n",
        "predictions = best_model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)  # Convert to class labels\n",
        "\n",
        "# Convert the one-hot encoded y_test back to class labels\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Display some predictions vs true labels\n",
        "print(\"\\nPredictions vs True Labels:\")\n",
        "for i in range(10):  # Display first 10 predictions\n",
        "    print(f\"Predicted: {predicted_classes[i]}, True: {true_classes[i]}\")\n",
        "\n",
        "# Plot training metrics\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(history)"
      ],
      "metadata": {
        "id": "wWJM7yI-6zAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved **100% test accuracy** with a **test loss of 0.0413**, indicating excellent performance on the test set. The **optimal hyperparameters** for the model were:\n",
        "\n",
        "- **Input Layer Units**: 32\n",
        "- **Hidden Layer Units**: 64\n",
        "- **Dropout Rate**: 0.2\n",
        "- **Learning Rate**: 0.006\n",
        "\n",
        "The predictions made by the model on the test set matched the true labels perfectly for the first 10 test samples, showing the model's ability to classify correctly across different classes (0, 1, and 2) in the Iris dataset."
      ],
      "metadata": {
        "id": "3tg_gr7H76y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code - Explaination\n",
        "\n",
        "Import Required Libraries:\n",
        "\n",
        "Imports necessary libraries such as TensorFlow, Keras, scikit-learn, and Matplotlib for model building, data preprocessing, and visualization.\n",
        "\n",
        "Load IRIS Dataset:\n",
        "\n",
        "Loads the IRIS dataset using load_iris() from sklearn.datasets and assigns features (X) and labels (y).\n",
        "\n",
        "Preprocess Labels:\n",
        "\n",
        "Encodes labels into one-hot encoding using LabelEncoder and to_categorical for multi-class classification.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Visualizes feature distributions of the IRIS dataset using a boxplot for data inspection.\n",
        "\n",
        "Train-Test Split:\n",
        "\n",
        "Splits the dataset into training (80%) and testing (20%) sets using train_test_split from sklearn.\n",
        "\n",
        "Feature Normalization:\n",
        "\n",
        "Normalizes the features using StandardScaler to improve model convergence during training.\n",
        "\n",
        "Model Definition for Hyperparameter Tuning:\n",
        "\n",
        "Defines a function build_model that takes hyperparameters as input. It builds a neural network with:\n",
        "\n",
        "An input layer with tunable neurons.\n",
        "\n",
        "A hidden layer with tunable neurons.\n",
        "\n",
        "A dropout layer for regularization.\n",
        "\n",
        "An output layer with softmax activation for multi-class classification.\n",
        "\n",
        "Hyperparameter Search Initialization:\n",
        "\n",
        "Initializes Keras Tuner with Hyperband strategy to perform hyperparameter optimization over specified ranges.\n",
        "\n",
        "Early Stopping Callback:\n",
        "\n",
        "Defines an early stopping callback to prevent overfitting, halting training when validation loss does not improve.\n",
        "\n",
        "Hyperparameter Search:\n",
        "\n",
        "Searches for the best hyperparameters using the tuner.search() method, based on validation accuracy.\n",
        "\n",
        "Retrieve Best Hyperparameters:\n",
        "\n",
        "After tuning, retrieves the optimal hyperparameters for the best model.\n",
        "\n",
        "Model Training:\n",
        "\n",
        "Builds and trains the model using the best-found hyperparameters, with the early stopping callback to monitor validation loss.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Evaluates the model on the test set and prints the test accuracy and loss.\n",
        "\n",
        "Predictions and True Labels Comparison:\n",
        "\n",
        "Makes predictions on the test set, converts the predictions from one-hot encoding to class labels, and compares the predictions with true labels.\n",
        "\n",
        "Metrics Visualization:\n",
        "\n",
        "Plots the training and validation accuracy and loss across epochs to visualize model performance."
      ],
      "metadata": {
        "id": "IuXAqsdB-9uQ"
      }
    }
  ]
}